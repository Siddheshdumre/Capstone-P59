import pandas as pd
import numpy as np
import joblib
import argparse
import os

def attribution_attack(model, X, hide_feature, attack_strength=0.05):
    """
    Performs a simple attribution manipulation attack.
    It adds noise to all features *except* the one we want to hide,
    making the other features seem more important.
    
    Args:
        model: The trained black-box model pipeline.
        X (pd.DataFrame): The input data to attack.
        hide_feature (str): The name of the feature to make less important.
        attack_strength (float): The scale of noise to add to other features.
        
    Returns:
        pd.DataFrame: The attacked (perturbed) data.
        np.ndarray: The model's predictions on the attacked data.
        np.ndarray: The model's logits (or probabilities) on the attacked data.
    """
    X_attacked = X.copy()
    
    features_to_attack = [
        col for col in X.select_dtypes(include=np.number).columns
        if col != hide_feature
    ]
    
    if not features_to_attack:
        print(f"⚠️ Warning: No numeric features to attack other than '{hide_feature}'.")
        predictions = model.predict(X_attacked)
        logits = model.predict_proba(X_attacked)
        return X_attacked, predictions, logits

    print(f"Attacking {len(features_to_attack)} features to hide '{hide_feature}'...")
    
    for col in features_to_attack:
        noise = np.random.normal(loc=0, scale=X[col].std() * attack_strength, size=X.shape[0])
        X_attacked[col] = X_attacked[col] + noise

    predictions = model.predict(X_attacked)
    logits = model.predict_proba(X_attacked)
    
    return X_attacked, predictions, logits

def main():
    parser = argparse.ArgumentParser(description="Run an Attribution Manipulation attack.")
    parser.add_argument('--model_path', type=str, required=True, help='Path to the trained model .joblib file.')
    parser.add_argument('--data_path', type=str, required=True, help='Path to the test data .csv file.')
    parser.add_argument('--output_dir', type=str, required=True, help='Directory to save the attacked data and predictions.')
    parser.add_argument('--hide_feature', type=str, required=True, help='The feature to make less important in explanations.')
    parser.add_argument('--strength', type=float, default=0.05, help='Strength of the attack (noise scale).')
    args = parser.parse_args()

    print(f"Loading model from {args.model_path}...")
    model = joblib.load(args.model_path)
    
    print(f"Loading data from {args.data_path}...")
    # The data_path now directly points to the features (X_test)
    X_test = pd.read_csv(args.data_path)
    
    if args.hide_feature not in X_test.columns:
        print(f"❌ Error: Feature '{args.hide_feature}' not found in the dataset.")
        return
        
    print("Applying Attribution Manipulation attack...")
    X_attacked, predictions, logits = attribution_attack(model, X_test, args.hide_feature, args.strength)
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    attacked_data_path = os.path.join(args.output_dir, 'attacked_data.csv')
    predictions_path = os.path.join(args.output_dir, 'predictions.npz')
    
    X_attacked.to_csv(attacked_data_path, index=False)
    np.savez(predictions_path, preds=predictions, logits=logits)
    
    print(f"✅ Attack complete. Results saved in {args.output_dir}")

if __name__ == '__main__':
    main()
