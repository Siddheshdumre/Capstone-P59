# -*- coding: utf-8 -*-
"""Attack Simulation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Vz3bOu3rYGOvKluVwG9qfis1ibi03eF
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone repo
!git clone https://github.com/Siddheshdumre/Capstone-P59.git
# %cd Capstone-P59

# Install deps (adjust as needed
!pip install -q numpy pandas scikit-learn joblib shap lime scipy matplotlib

!mkdir -p ./data/compas

# Create a directory for the raw data
!mkdir -p ./downloads

# Download the COMPAS dataset from the ProPublica repository
!wget -O ./downloads/compas-scores-two-years.csv https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv

import nbformat
import os

notebook_path = 'train_compas.ipynb'
old_line = 'OUTPUT_DIR="/content/drive/MyDrive/model_outputs/compas"'
new_line = 'OUTPUT_DIR="./data/compas"'

# Check if the file exists
if not os.path.exists(notebook_path):
    print(f"Error: The file {notebook_path} was not found.")
else:
    # Read the notebook
    with open(notebook_path, 'r', encoding='utf-8') as f:
        nb = nbformat.read(f, as_version=4)

    # Find and replace the line in the code cells
    line_found_and_replaced = False
    for cell in nb.cells:
        if cell.cell_type == 'code':
            # Check for both versions with and without escaped quotes
            if old_line in cell.source or old_line.replace('"', '\\"') in cell.source:
                cell.source = cell.source.replace(old_line, new_line)
                cell.source = cell.source.replace(old_line.replace('"', '\\"'), new_line.replace('"', '\\"'))
                line_found_and_replaced = True
                break # Stop after finding the first instance

    # Write the changes back to the notebook file
    if line_found_and_replaced:
        with open(notebook_path, 'w', encoding='utf-8') as f:
            nbformat.write(nb, f)
        print(f"✅ Successfully updated the OUTPUT_DIR in {notebook_path}.")
    else:
        print(f"⚠️  Could not find the line '{old_line}' in {notebook_path}. It might have already been changed.")

import nbformat
import os

notebook_path = 'train_compas.ipynb'

# Define the paths to find and replace
paths_to_replace = {
    # Path for the output directory
    'OUTPUT_DIR="/content/drive/MyDrive/model_outputs/compas"': 'OUTPUT_DIR="./data/compas"',
    # Path for the input data
    "DATA_PATH='/content/drive/MyDrive/datasets/compas.csv'": "DATA_PATH='./downloads/compas-scores-two-years.csv'"
}

# Check if the file exists
if not os.path.exists(notebook_path):
    print(f"Error: The file {notebook_path} was not found.")
else:
    # Read the notebook
    with open(notebook_path, 'r', encoding='utf-8') as f:
        nb = nbformat.read(f, as_version=4)

    replacements_made = 0
    # Loop through each path replacement rule
    for old_line, new_line in paths_to_replace.items():
        line_found_and_replaced = False
        for cell in nb.cells:
            if cell.cell_type == 'code':
                # Check for both versions with and without escaped quotes
                if old_line in cell.source or old_line.replace('"', '\\"').replace("'", "\\'") in cell.source:
                    cell.source = cell.source.replace(old_line, new_line)
                    cell.source = cell.source.replace(old_line.replace('"', '\\"').replace("'", "\\'"), new_line.replace('"', '\\"'))
                    line_found_and_replaced = True
                    break # Stop after finding the first instance

        if line_found_and_replaced:
            replacements_made += 1

    # Write the changes back to the notebook file
    if replacements_made > 0:
        with open(notebook_path, 'w', encoding='utf-8') as f:
            nbformat.write(nb, f)
        print(f"✅ Successfully made {replacements_made} path update(s) in {notebook_path}.")
    else:
        print(f"⚠️  Could not find any paths to update in {notebook_path}. They might have already been changed.")

import nbformat
import os

# --- Step 1: Ensure the dataset is downloaded ---
print("--- Step 1: Downloading dataset ---")
os.makedirs('./downloads', exist_ok=True)
if not os.path.exists('./downloads/compas-scores-two-years.csv'):
    get_ipython().system('wget -q -O ./downloads/compas-scores-two-years.csv https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv')
    print("Dataset downloaded successfully.")
else:
    print("Dataset already exists.")


# --- Step 2: Force-update the notebook paths ---
print("\n--- Step 2: Updating paths in train_compas.ipynb ---")
notebook_path = 'train_compas.ipynb'
new_data_path = './downloads/compas-scores-two-years.csv'
new_output_dir = './data/compas'

if not os.path.exists(notebook_path):
    print(f"❌ Error: The file {notebook_path} was not found.")
else:
    with open(notebook_path, 'r', encoding='utf-8') as f:
        nb = nbformat.read(f, as_version=4)

    replacements_made = 0
    for cell in nb.cells:
        if cell.cell_type == 'code':
            lines = cell.source.split('\n')
            new_lines = []
            for line in lines:
                if line.strip().startswith("DATA_PATH"):
                    new_line = f"DATA_PATH = '{new_data_path}'"
                    if line.strip() != new_line:
                         print(f"Updating data path...")
                         replacements_made += 1
                    new_lines.append(new_line)
                elif line.strip().startswith("OUTPUT_DIR"):
                    new_line = f"OUTPUT_DIR = '{new_output_dir}'"
                    if line.strip() != new_line:
                        print(f"Updating output directory...")
                        replacements_made += 1
                    new_lines.append(new_line)
                else:
                    new_lines.append(line)
            cell.source = '\n'.join(new_lines)

    with open(notebook_path, 'w', encoding='utf-8') as f:
        nbformat.write(nb, f)

    if replacements_made > 0:
        print(f"✅ Paths updated successfully.")
    else:
        print("✅ Paths were already correct.")


# --- Step 3: Execute the updated notebook ---
print("\n--- Step 3: Running the train_compas.ipynb notebook ---")
get_ipython().run_line_magic('run', 'train_compas.ipynb')

print("\n--- All steps complete! ---")

# Create folders
!mkdir -p data/adult
!mkdir -p data/german

# Unzip datasets into their folders
!unzip -q adult-20251017T181359Z-1-001.zip -d data/adult
!unzip -q german_credit_risk-20251017T181409Z-1-001.zip -d data/german

# Check that extraction worked
!ls data/adult
!ls data/german

# look for common save calls inside notebooks
!grep -R --line-number "joblib.dump" -n .
!grep -R --line-number "pickle.dump" -n .
!grep -R --line-number "to_pickle(" -n .

# show lines that define OUTPUT_DIR in notebooks
!grep -n "OUTPUT_DIR" -n train_*.ipynb || true

# fallback: list any .joblib files in the repo
!find . -type f -name "*.joblib" -maxdepth 4 -print

# search for all saved model files in your repo
!find . -type f -name "*.joblib"

# smoke_test_load.py
import joblib, pandas as pd
model_path = "data/adult/adult/mlp.joblib"   # adjust per model below
print("Loading:", model_path)
model = joblib.load(model_path)
print("Loaded model type:", type(model))

# quick test: load a small sample (replace with valid CSV from repo if present)
# If no test CSV exists, just print model steps:
try:
    print("Pipeline steps:", getattr(model, "named_steps", "no named_steps"))
except Exception:
    pass

import subprocess

subprocess.run([
    "python", "attacks/off_manifold_simple.py",
    "--model", "data/adult/adult/mlp.joblib",
    "--input", "data/adult/adult/test.csv",
    "--out", "attacked/adult/mlp_offmanifold_scale2"
])

!pwd

!ls

!cd sample_data

!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd Capstone-P59

!python generate_adult_test_set.py

!python generate_german_test_set.py

!pip install adversarial-robustness-toolbox

# --- Off-Manifold Attacks ---
!python attacks/off-manifold-attack --model_path ./data/adult/adult/mlp.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/mlp_offmanifold
!python attacks/off-manifold-attack --model_path ./data/adult/adult/rf.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/rf_offmanifold
!python attacks/off-manifold-attack --model_path ./data/adult/adult/lr.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/lr_offmanifold

# --- Surrogate Attacks ---
!python attacks/surrogate-model-attack --model_path ./data/adult/adult/mlp.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/mlp_surrogate
!python attacks/surrogate-model-attack --model_path ./data/adult/adult/rf.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/rf_surrogate
!python attacks/surrogate-model-attack --model_path ./data/adult/adult/lr.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/lr_surrogate

# --- Attribution Attacks ---
!python attacks/attribution-manipulation --model_path ./data/adult/adult/mlp.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/mlp_attribution_age --hide_feature age
!python attacks/attribution-manipulation --model_path ./data/adult/adult/rf.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/rf_attribution_age --hide_feature age
!python attacks/attribution-manipulation --model_path ./data/adult/adult/lr.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/lr_attribution_age --hide_feature age

# --- Attribution Attacks ---
!python attacks/attribution-manipulation --model_path ./data/adult/adult/mlp.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/mlp_attribution_age --hide_feature age
!python attacks/attribution-manipulation --model_path ./data/adult/adult/rf.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/rf_attribution_age --hide_feature age
!python attacks/attribution-manipulation --model_path ./data/adult/adult/lr.joblib --data_path ./data/adult/adult/test.csv --output_dir ./attacked/adult/lr_attribution_age --hide_feature age

#german credit risk dataset
# --- Off-Manifold Attacks ---
!python attacks/manifold.py --model_path ./data/german/german_credit_risk/mlp.joblib --data_path ./data/german/german_credit_risk/test.csv --output_dir ./attacked/german/mlp_offmanifold
!python attacks/manifold.py --model_path ./data/german/german_credit_risk/rf.joblib --data_path ./data/german/german_credit_risk/test.csv --output_dir ./attacked/german/rf_offmanifold
!python attacks/manifold.py --model_path ./data/german/german_credit_risk/lr.joblib --data_path ./data/german/german_credit_risk/test.csv --output_dir ./attacked/german/lr_offmanifold

# --- Surrogate Attacks ---
!python attacks/surrogate.py --model_path ./data/german/german_credit_risk/mlp.joblib --data_path ./data/german/german_credit_risk/test.csv --output_dir ./attacked/german/mlp_surrogate
!python attacks/surrogate.py --model_path ./data/german/german_credit_risk/rf.joblib --data_path ./data/german/german_credit_risk/test.csv --output_dir ./attacked/german/rf_surrogate
!python attacks/surrogate.py --model_path ./data/german/german_credit_risk/lr.joblib --data_path ./data/german/german_credit_risk/test.csv --output_dir ./attacked/german/lr_surrogate

# --- Attribution Attacks (Hiding 'age') ---
!python attacks/attribution.py --model_path ./data/german/german_credit_risk/mlp.joblib --data_path ./data/german/german_credit_risk/german_credit_risk/test.csv --output_dir ./attacked/german/mlp_attribution_age --hide_feature age
!python attacks/attribution.py --model_path ./data/german/german_credit_risk/rf.joblib --data_path ./data/german/german_credit_risk/german_credit_risk/test.csv --output_dir ./attacked/german/rf_attribution_age --hide_feature age
!python attacks/attribution.py --model_path ./data/german/german_credit_risk/lr.joblib --data_path ./data/german/german_credit_risk/german_credit_risk/test.csv --output_dir ./attacked/german/lr_attribution_age --hide_feature age

#compasdataset
# --- Off-Manifold Attacks ---
!python attacks/off-manifold-attack --model_path ./data/compas/mlp.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/mlp_offmanifold
!python attacks/off-manifold-attack --model_path ./data/compas/rf.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/rf_offmanifold
!python attacks/off-manifold-attack --model_path ./data/compas/lr.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/lr_offmanifold

# --- Surrogate Attacks ---
!python attacks/surrogate-model-attack --model_path ./data/compas/mlp.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/mlp_surrogate
!python attacks/surrogate-model-attack --model_path ./data/compas/rf.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/rf_surrogate
!python attacks/surrogate-model-attack --model_path ./data/compas/lr.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/lr_surrogate

# --- Attribution Attacks (Hiding 'age') ---
!python attacks/attribution-manipulation --model_path ./data/compas/mlp.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/mlp_attribution_age --hide_feature age
!python attacks/attribution-manipulation --model_path ./data/compas/rf.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/rf_attribution_age --hide_feature age
!python attacks/attribution-manipulation --model_path ./data/compas/lr.joblib --data_path ./data/compas/test.csv --output_dir ./attacked/compas/lr_attribution_age --hide_feature age

import os
import zipfile
import pandas as pd

def prepare_german_test_set(zip_path, output_dir):
    """Prepares the German Credit Risk test dataset."""
    print("--- Preparing German Credit dataset ---")
    extracted_folder_name = "german_credit_risk" # Assuming the folder inside the zip is named this
    extracted_data_path = os.path.join(output_dir, extracted_folder_name)
    os.makedirs(extracted_data_path, exist_ok=True)

    # Find the zip file if the exact name is not provided
    if not os.path.exists(zip_path):
        print(f"Zip file not found at {zip_path}. Searching in the current directory...")
        zip_files = [f for f in os.listdir('.') if f.startswith('german_credit_risk-') and f.endswith('.zip')]
        if len(zip_files) == 1:
            zip_path = zip_files[0]
            print(f"Found zip file: {zip_path}")
        else:
            print("❌ Error preparing German test set: Could not find the german credit risk zip file.")
            return

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(output_dir)
        print(f"Unzipping {zip_path}...")

        # Assuming the data file is named 'german.data' inside the extracted folder
        raw_data_path = os.path.join(extracted_data_path, 'german.data')
        if not os.path.exists(raw_data_path):
             # try to find the file inside the extracted folder
             extracted_files = os.listdir(extracted_data_path)
             data_files = [f for f in extracted_files if f.endswith('.data') or f.endswith('.csv')]
             if len(data_files) == 1:
                  raw_data_path = os.path.join(extracted_data_path, data_files[0])
                  print(f"Found data file: {raw_data_path}")
             else:
                print(f"❌ Error preparing German test set: Could not find 'german.data' or similar file in {extracted_data_path}.")
                return


        # Load and process the data (assuming it's space-separated and needs a header)
        # Define columns based on the dataset description (from UCI Machine Learning Repository)
        column_names = ['Checking account', 'Duration', 'Credit history', 'Purpose', 'Credit amount',
                        'Saving accounts', 'Employment', 'Installment rate', 'Personal status',
                        'Other debtors', 'Residence since', 'Property', 'Age', 'Other installment plans',
                        'Housing', 'Number of existing credits', 'Job', 'Number of people liable',
                        'Telephone', 'Foreign worker', 'Credit risk']
        df = pd.read_csv(raw_data_path, sep=' ', header=None, names=column_names)

        # The last column is the target (1: Good, 2: Bad). Convert to 0 (Good) and 1 (Bad)
        df['Credit risk'] = df['Credit risk'].apply(lambda x: 0 if x == 1 else 1)

        # Save the processed data as test.csv
        output_test_path = os.path.join(extracted_data_path, 'test.csv')
        df.to_csv(output_test_path, index=False)
        print(f"✅ Successfully created '{output_test_path}'.")

    except Exception as e:
        print(f"❌ Error preparing German test set: {e}")

def prepare_compas_test_set(data_path, output_dir):
    """Prepares the COMPAS test dataset."""
    print("\n--- Preparing COMPAS dataset ---")
    output_test_path = os.path.join(output_dir, 'test.csv')

    if os.path.exists(output_test_path):
        print("COMPAS test data already exists.")
        return

    try:
        print("Loading and preprocessing COMPAS data...")
        df = pd.read_csv(data_path)

        # Drop unused features
        df = df[[
            'sex', 'age', 'age_cat', 'race', 'juv_fel_count', 'juv_misd_count',
            'juv_other_count', 'priors_count', 'c_charge_degree', 'c_charge_desc',
            'is_recid', 'two_year_recid', 'days_since_arrest', 'decile_score'
        ]]

        # Select relevant features for the model (based on typical usage, adjust if needed)
        features = ['sex', 'age', 'age_cat', 'race', 'juv_fel_count', 'juv_misd_count',
                    'juv_other_count', 'priors_count', 'c_charge_degree', 'c_charge_desc']
        target = 'two_year_recid'

        # Assuming you want to save the features and target for the test set
        test_df = df[features + [target]]

        test_df.to_csv(output_test_path, index=False)
        print(f"✅ Successfully created '{output_test_path}'")

    except FileNotFoundError:
        print(f"❌ Error preparing COMPAS test set: Data file not found at {data_path}")
    except Exception as e:
        print(f"❌ Error preparing COMPAS test set: {e}")


if __name__ == "__main__":
    # Assuming zip files are in the current directory and data/compas and data/german_credit_risk exist
    german_zip_path = 'german_credit_risk-20251017T181409Z-1-001.zip'
    german_output_dir = './data/german' # This should be the directory where the zip is extracted to

    compas_data_path = './downloads/compas-scores-two-years.csv' # Assuming this is already downloaded
    compas_output_dir = './data/compas'

    # Ensure output directories exist
    os.makedirs(german_output_dir, exist_ok=True)
    os.makedirs(compas_output_dir, exist_ok=True)

    prepare_german_test_set(german_zip_path, german_output_dir)
    prepare_compas_test_set(compas_data_path, compas_output_dir)

    print("\n--- All test data prepared. ---")

# Commented out IPython magic to ensure Python compatibility.
# Make sure you are in the /content/ directory first
# %cd /content/

# Rename your existing folder
!mv Capstone-P59 Capstone-P59_with_attacks

!git clone https://github.com/Siddheshdumre/Capstone-P59.git

!cp -r /content/Capstone-P59_with_attacks/attacks/ /content/Capstone-P59/

# Commented out IPython magic to ensure Python compatibility.
# Navigate into the new repository directory
# %cd /content/Capstone-P59

# Tell Git to track your results folder
!git add attacked/

# Save your work with a descriptive message
!git commit -m "feat: Add successful attack simulation results for Adult dataset"

# Push your changes to the remote repository
!git push origin main

!git config --global user.email "padmawarvaidehi123@gmail.com"
!git config --global user.name "Vaidehi"

!ssh-keygen -t rsa -b 4096 -C "padmawarvaidehi123@gmail.com"

!cat /content/Capstone-P59/downloads.pub

!git remote set-url origin git@github.com:Siddheshdumre/Capstone-P59.git

# Commented out IPython magic to ensure Python compatibility.
# --- STEP 1: PREPARE YOUR RESULTS (SAFETY COPY) ---
print("--- Backing up your results... ---")
# Make sure we are in the base /content/ directory
# %cd /content/
# Rename your current work folder to keep it safe
!mv -f Capstone-P59 Capstone-P59_with_my_results


# --- STEP 2: SECURELY CONFIGURE YOUR UPLOADED SSH KEY ---
print("\n--- Configuring your existing SSH Key... ---")
# Create the .ssh directory
!mkdir -p /root/.ssh/
# Move your uploaded private key to the correct location
!mv /content/id_rsa /root/.ssh/id_rsa
# CRITICAL: Set the required secure permissions for the private key
!chmod 600 /root/.ssh/id_rsa


# --- STEP 3: PREPARE THE REPOSITORY FOR PUSHING ---
print("\n--- Preparing the repository... ---")
# Clone a fresh copy of the repository
!git clone https://github.com/Siddheshdumre/Capstone-P59.git
# Copy ONLY your 'attacked' folder into the new, clean repository
!cp -r /content/Capstone-P59_with_my_results/attacked/ /content/Capstone-P59/
# Navigate into the new repository directory
# %cd /content/Capstone-P59


# --- STEP 4: PUSH YOUR WORK USING YOUR SSH KEY ---
print("\n--- Pushing your work to GitHub... ---")
# Configure Git to use SSH and trust GitHub's host key
!git remote set-url origin git@github.com:Siddheshdumre/Capstone-P59.git
!ssh-keyscan github.com >> /root/.ssh/known_hosts && chmod 644 /root/.ssh/known_hosts

# Add, commit, and push your final work.
# This will now use the private key you uploaded.
!git add attacked/
!git commit -m "feat: Add successful attack simulation results for Adult dataset"
!git push origin main

# Commented out IPython magic to ensure Python compatibility.
# First, navigate to the correct parent directory
# %cd /content/

# Now, create the zip file. This command will archive the Capstone-P59 folder.
!zip -r project_download.zip Capstone-P59_with_attacks

